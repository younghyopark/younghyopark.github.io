<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Younghyo Park</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2RCCCZEBV7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-2RCCCZEBV7');
  </script>

  <meta name="author" content="Younghyo Park">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦¾</text></svg>">

  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:site" content="@younghyo_park" />
  <meta name="twitter:title" content="Younghyo Park" />
  <meta name="twitter:description" content="Robotics PhD student at MIT CSAIL" />
  <meta name="twitter:image" content="https://raw.githubusercontent.com/younghyopark/younghyopark.github.io/new/images/IMG_0524.jpeg" />

  <meta property="og:type" content="profile" />
  <meta property="og:title" content="Younghyo Park" />
  <meta property="og:description" content="Robotics PhD student at MIT CSAIL" />
  <meta property="og:url" content="https://younghyopark.me/" />
  <meta property="og:image" content="https://raw.githubusercontent.com/younghyopark/younghyopark.github.io/new/images/IMG_0524.jpeg" />    

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Younghyo Park</name>
              </p>
              <p>I am a second-year Ph.D. student in <a href="https://eecs.mit.edu/">EECS</a> at <a href="https://csail.mit.edu/">MIT CSAIL</a>, 
                advised by Professor <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>.
	            </p>

              <p>I received my Bachelors degree (Summa Cum Laude) in Mechanical Engineering at <a href="https://snu.ac.kr">Seoul National University</a>.
                  
              Previously, I was a full-time research scientist at <a href="https://naverlabs.com/en/">NAVER LABS</a>, 
                developing machine/reinforcement learning algorithms to make robot arms like 
              <a href="https://www.naverlabs.com/en/ambidex">AMBIDEX</a> perform various daily tasks. I also spent some time at <a href="https://saige.ai/">Saige Research</a> as an undergraduate research intern.</p>
              <!-- <p>I will soon be starting my Ph.D. program at <a href="https://csail.mit.edu/">MIT CSAIL</a> starting 2023 Fall, 
                working with Professor <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>. 
                I received my bachelors degree (Summa Cum Laude) in Mechanical Engineering at Seoul National University.</p> -->
              
              <p style="text-align:center">
                <a href="mailto:younghyo@mit.edu">Email</a> &nbsp/&nbsp
                <a href="data/Younghyo_Park_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=eEYGjdMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/younghyo_park">Twitter</a> 
              </p>

            </td>
            <td style="padding:2.0%;width:30%;max-width:30%">
              <a><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_0524.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-15px"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Vision</heading>
              <p>
                My dream is to see robots performing all sorts of complex, 
                long-horizon and contact-rich manipulation tasks in real-world, just like we humans do everyday. 
                One <strong>scalable</strong> way to acheive this dream is to pretrain robots with  
                (a) a repertoire of reusable low-level motor control skills and 
                (b) an intelligence that can temporally or spatially compose such skills to complete any given unseen tasks. 
              </p>
              <p>
                What kind of low-level manipulation skills should we pretrain in advance, and how should we train them? 
                With such reusable repertoire of skills, how should we train an intelligence 
                that can orchestrate them to perform longer-horizon tasks? My upcoming works will mainly address these questions! </p>
                <p>
                  * denotes equal contribution.
                </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;margin-bottom:-25px"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/Improbable-AI/VisionProTeleop">
                <papertitle>[2025 IAP]  Modern-Robot Learning: Hands-on Tutorial</papertitle>
              </a>
              <br>
              <strong>Younghyo Park</strong>, <a href="https://fang-haoshu.github.io/">Haoshu Fang</a>, <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
              <br>
              <a href="https://modern-robot-learning.github.io/">course website (6.S186)</a>
              /
              <a href="https://youtube.com/playlist?list=PLikvwLlAc2PhZWoA1D5x29bl-fB6uaeJY&si=SBOJmEe9IpmE3_N8">lecture videos</a>
              <p></p>
              <p>
                This course provides a practical introduction to training robots using data-driven methods. 
                Key topics include data collection methods for robotics, policy training methods, and using simulated environments for robot learning. 
                Throughout the course, students will have hands-on experience to collect robot data, train policies, and evaluate its performance. 
              </p>
            </td>
          </tr>
        </tbody>
      </table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;margin-bottom:-25px"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Open-source Projects</heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
        <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="images/personal_website_visionpro_2.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='images/visionpro_still.png' width="100%" >
            </div>
            <script type="text/javascript">
              function zipnerf_start() {
                document.getElementById('zipnerf_image').style.opacity = "1";
              }
    
              function zipnerf_stop() {
                document.getElementById('zipnerf_image').style.opacity = "0";
              }
              zipnerf_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/Improbable-AI/VisionProTeleop">
              <papertitle>Using Apple Vision Pro to Control and Train Robots</papertitle>
            </a>
            <br>
            <strong>Younghyo Park</strong>, <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
            
            <br>
            <a href="https://github.com/Improbable-AI/VisionProTeleop">github</a>
            /
            <a href="https://apps.apple.com/us/app/tracking-streamer/id6478969032">App Store</a>
            /
            <a href="https://x.com/younghyo_park/status/1766274298422161830?s=20">twitter</a>
            /
            <a href="data/using_vision_pro_to_control_and_train_robots.pdf">short paper</a>
            <p></p>
            <p>
            An app for Apple Vision Pro that can stream user's head / wrist / finger tracking results to any machines connected to the same network. This can be used to (a) teleoperate robots using human motions and (b) collect datasets of humans navigating and manipulating the real-world!
            </p>
          </td>
        </tr>
          </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-25px"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
    </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				

      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/dart_personal_web 1_31.50.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/env_shaping_web.png' width="100%" >
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }
            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dexhub.ai/project">
            <papertitle>DexHub and DART: Infrastructure for Internet-Scale Robot Data Collection</papertitle>
          </a>
          <br>
          <strong>Younghyo Park</strong>, <a href="mailto:jagdeep@mit.edu">Jagdeep Bhatia</a>,  <a href="mailto:ankile@mit.edu">Lars Ankile</a>,  <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
          <br>
          <em>arXiv</em>, 2024 
          <br>
          <a href="https://dexhub.ai/project">project page</a>
          /
          <a href="https://x.com/younghyo_park/status/1815986042098348163">twitter</a>
          <p></p>
          <p>
            DART is a teleoperation platform that leverages cloud-based simulation and augmented reality (AR) to revolutionize robotic data collection. 
            It enables higher data collection throughput with reduced physical fatigue and facilitates robust policy transfer to real-world scenarios. 
            All datasets are stored in the DexHub cloud database, providing an ever-growing resource for robot learning.
          </p>
        </td>
      </tr>





      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/SASD_personal_web 1_rf30.00.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div> -->
            <img src='images/env_shaping_web.png' width="100%" >
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }
            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://auto-env-shaping.github.io/">
            <papertitle>Position: Automatic Environment Shaping is the Next Frontier in RL</papertitle>
          </a>
          <br>
          <strong>Younghyo Park*</strong>, <a href="mailto:gmargo@mit.edu">Gabriel Margolis*</a>, <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
          <br>
          <em>ICML</em>, 2024 <font color="red"> <strong>(Oral Presentation, Top 5%)</strong> </font>
          <br>
          <a href="https://auto-env-shaping.github.io/">project page</a>
          /
          <a href="https://youtu.be/wLfRDlVcOWM?si=QQokTXpO9gq3lZn4">video</a>
          /
          <a href="https://x.com/younghyo_park/status/1815986042098348163">twitter</a>
          <p></p>
          <p>
            Most robotics practitioners spend most time shaping the environments (e.g. rewards, observation/action spaces, low-level controllers, simulation dynamics) than to tune RL algorithms to obtain a desirable controller. We posit that the community should focus more on (a) automating environment shaping procedures and/or (b) developing stronger RL algorithms that can tackle unshaped environments.
          </p>
        </td>
      </tr>
  





      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/SASD_personal_web 1_rf30.00.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/SASD_still2.jpeg' width="100%" >
        </div>
        <script type="text/javascript">
          function zipnerf_start() {
            document.getElementById('zipnerf_image').style.opacity = "1";
          }

          function zipnerf_stop() {
            document.getElementById('zipnerf_image').style.opacity = "0";
          }
          zipnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://safe-skill.github.io/">
          <papertitle>Safety-Aware Unsupervised Skill Discovery</papertitle>
        </a>
        <br>
        <a href="mailto:sunin.kim@naverlabs.com">Sunin Kim</a>*, <a href="https://scholar.google.com/citations?user=aa1mE8cAAAAJ&hl=en&oi=ao">Jaewoon Kwon</a>*, <a href="https://scholar.google.co.kr/citations?user=ZBGLua0AAAAJ&hl=en">Taeyoon Lee</a>*, 
        <strong>Younghyo Park*</strong>, <a href="https://scholar.google.com/citations?user=XneKjCsAAAAJ&hl=en&oi=ao">Julien Perez</a>
        <br>
        <em>ICRA</em>, 2023
        <br>
        <a href="https://safe-skill.github.io/">project page</a>
        /
        <a href="https://youtu.be/h58P6czMQh4">video</a>
        /
        <a href="https://twitter.com/younghyo_park/status/1655438387900481537">twitter</a>
        <p></p>
        <p>
        An algorithm that can discover diverse and useful set of skills from scratch that is inherently safe to be composed for unseen downstream tasks. 
        Considering safety during skill discovery phase is a must when solving safety-critical downstream tasks.
        </p>
      </td>
    </tr>
    <!-- bgcolor="#ffffd0" -->
		
    <tr onmouseout="db3d_stop()" onmouseover="db3d_start()" >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/ARTO_personal_web 1_rf30.00.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/ARTO_personal_still.jpeg' width="160">
        </div>
        <script type="text/javascript">
          function db3d_start() {
            document.getElementById('db3d_image').style.opacity = "1";
          }

          function db3d_stop() {
            document.getElementById('db3d_image').style.opacity = "0";
          }
          db3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
				<a href="https://drawing-robot.github.io/">
          <papertitle>Robot Learning to Paint from Demonstrations</papertitle>
        </a>
        <br>
        <strong>Younghyo Park*</strong>, <a href="https://www.linkedin.com/in/seunghun-jeon-559a971b2">Seunghoon Jeon</a>*, <a href="https://scholar.google.co.kr/citations?user=ZBGLua0AAAAJ&hl=en">Taeyoon Lee</a>  <br>
        <em>IROS</em>, 2022 <font color="red"><strong>(Winner: Best Entertainment & Amusement Paper Award)</strong></font>
        <br>
				<a href="https://drawing-robot.github.io/">project page</a> / 
        <a href="https://youtu.be/2DCkyE0l0aI">video</a> /
				<a href="https://naverlabs.com/storyDetail/245">story</a> /
				<a href="https://youtu.be/7z-S1Bu6XPo">interview</a> / 
				<a href="https://ieeexplore.ieee.org/abstract/document/9981633">paper</a>
        <p></p>
        <p>Drawing robot ARTO-1 performs complex drawings in real-world by learning low-level stroke drawing skills, requiring delicate force control, from human demonstrations. This approach eases the planning required to actually perform an artistic drawing.</p>
      </td>
    </tr>

    

<tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='dreamfusion_image'>
        <img src='images/col_detection2.jpeg' width="160"></div>
    <img src='images/col_detection2.jpeg' width="160">
    </div>
    <script type="text/javascript">
      function dreamfusion_start() {
        document.getElementById('dreamfusion_image').style.opacity = "1";
      }

      function dreamfusion_stop() {
        document.getElementById('dreamfusion_image').style.opacity = "0";
      }
      dreamfusion_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://younghyopark.me/">
      <papertitle>Collision detection for robot manipulators using unsupervised anomaly detection algorithms
      </papertitle>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=f7Rr7tcAAAAJ&hl=en&oi=ao">Kyumin Park</a>,
    <strong>Younghyo Park</strong>, <a href="https://swyoon.github.io/">Sangwoong Yoon</a>, <a href="https://sites.google.com/robotics.snu.ac.kr/fcp/">Frank C. Park</a>
    <br>
    <em>Transactions on Mechatronics</em>, 2021 
    <br>
    <a href="https://ieeexplore.ieee.org/document/9600832">paper</a>
    <p></p>
    <p>
    We detect collisions for robot manipulators using unsupervised anomaly detection methods. Compared to supervised approach, this approach does not require collisions datasets and even detect unseen collision types. 
    </p>
  </td>
</tr>

<tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='alignerf_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/detection 1_rf30.00.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/detection-still.jpeg' width="160">
    </div>
    <script type="text/javascript">
      function alignerf_start() {
        document.getElementById('alignerf_image').style.opacity = "1";
      }

      function alignerf_stop() {
        document.getElementById('alignerf_image').style.opacity = "0";
      }
      alignerf_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://link.springer.com/chapter/10.1007/978-3-030-97672-9_26">
      <papertitle>Deep Learning Based Parking Slot Detection and Tracking: PSDT-Net</papertitle>
    </a>
    <br>
    <strong>Younghyo Park</strong>, <a href="https://scholar.google.co.kr/citations?user=j8EB4g0AAAAJ&hl=ko">Joonwoo Ahn</a>, <a href="https://scholar.google.co.kr/citations?user=XtKmE78AAAAJ&hl=en">Jaeheung Park</a>
    <br>
    <em>ICRITA</em>, 2022
    <br>
    <a href="https://link.springer.com/chapter/10.1007/978-3-030-97672-9_26">paper</a>
    <p></p>
    <p>
    Performs real-time parking slot detection and tracking for autonomous parking systems. 
    </p>
  </td>
</tr>


		  

        </tbody></table>

				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Check out Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">repository</a> for the template of this website.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
